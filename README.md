# Goal
Gain a greater understanding of how transformer-based language models work and what kind of NLP tasks they can be applied too. I will update this repository as new things come up.

## Notebooks
The markdown in the notebooks is where I explain everything and write down notes that I feel are relevant.
### The Basics
* [Implementing A Basic Transformer in PyTorch](#)

### Transfer Learning
* [Answering Questions With BERT](#)
* [Generating Text Using GPT-2](Generating_Text_Using_GPT_2.ipynb)

## Getting Started With NLP
Nascent list of tutorials, blog posts, and papers that establish a well rounded NLP knowledge basis. All of this stuff is the prerequisites to understanding transformers.
### Tutorials
* [Word Embeddings with PyTorch](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html)
* [Translation With A Sequence To Sequence Network and Attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)

### Blog Posts
* [Illustrated Guide to LSTM’s and GRU’s: A step by step explanation](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)
* [Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) - This post is really really good
* [Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)

### Papers
* [How to Read a Paper](https://web.stanford.edu/class/ee384m/Handouts/HowtoReadPaper.pdf) - If you have never read this I highly recommend it
* [Sequence Transduction with Recurrent Neural Networks](https://arxiv.org/abs/1211.3711)
* [Sequence to Sequence Learning with Neural Networks](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html)
* [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)
* [Neural Machine Translation Jointly Learning To Align and Translate](https://arxiv.org/pdf/1409.0473.pdf) - Attention mechanism


## Transformers
Tutorials, blog posts, and papers that go in depth on how transformers work and why they are better than the previous language models using Recurrent Neural Networks.

### Tutorial
* [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html#encoder-and-decoder-stacks) - Attention Is All You Need with code and explanation

### Blog Posts
* [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)
* [Transformers in Natural Language Processing — A Brief Survey](https://eigenfoo.xyz/transformers-in-nlp/)


### Papers
* [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - The concept of the transformer was born here
