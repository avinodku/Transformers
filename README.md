## Getting Started With NLP
Nascent list of tutorials, blog posts, and papers that establish a well rounded (Natural Language Processing) NLP knowledge basis. All of this stuff is the prerequisites to understanding transformers.
### Tutorials
* [Word Embeddings with PyTorch](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html)
* [Translation With A Sequence To Sequence Network and Attention](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)

### Blog Posts
* [Illustrated Guide to LSTM’s and GRU’s: A step by step explanation](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)
* [Attention? Attention!](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html) - This post is really really good

### Papers
* [How to Read a Paper](https://web.stanford.edu/class/ee384m/Handouts/HowtoReadPaper.pdf) - If you have never read this I highly recommend it
* [Sequence Transduction with Recurrent Neural Networks](https://arxiv.org/abs/1211.3711)
* [Sequence to Sequence Learning with Neural Networks](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html)
* [Effective Approaches to Attention-based Neural Machine Translation](https://arxiv.org/abs/1508.04025)
* [Neural Machine Translation Jointly Learning To Align and Translate](https://arxiv.org/pdf/1409.0473.pdf) - Attention mechanism


## Transformers
Tutorials and papers that go in depth on how transformers work and why they are better than the previous language models using Recurrent Neural Networks.

### Tutorial
* [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html#encoder-and-decoder-stacks) - Attention Is All You Need with code

### Papers
* [Attention Is All You Need](https://arxiv.org/abs/1706.03762) - The concept of the transformers was born here
